{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4fec0206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /home/viktoriia/Documents/ML/project-25-2-scrum-team-data/notebooks\n",
      "DATA_PATH: /home/viktoriia/Documents/ML/project-25-2-scrum-team-data/notebooks/data/processed_product.pkl -> exists: False\n",
      "CHROMA_DIR: /home/viktoriia/Documents/ML/project-25-2-scrum-team-data/notebooks/chroma_db\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from vector_pipeline import settings\n",
    "\n",
    "# Detect project root (one level above notebooks/)\n",
    "project_root = Path(\"..\").resolve() if Path(\".\").name == \"notebooks\" else Path(\".\").resolve()\n",
    "print(\"Project root:\", project_root)\n",
    "\n",
    "# Override paths in settings to be absolute paths from project root\n",
    "settings.DATA_PATH = project_root / \"data\" / \"processed_product.pkl\"\n",
    "settings.CHROMA_DIR = project_root / \"chroma_db\"\n",
    "\n",
    "print(\"DATA_PATH:\", settings.DATA_PATH.resolve(), \"-> exists:\", settings.DATA_PATH.exists())\n",
    "print(\"CHROMA_DIR:\", settings.CHROMA_DIR.resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "264e20ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA_PATH as defined: data/processed_product.pkl\n",
      "Absolute path: /home/viktoriia/Documents/ML/project-25-2-scrum-team-data/notebooks/data/processed_product.pkl\n",
      "Exists? False\n",
      "\n",
      "Contents of ./data directory:\n",
      "No 'data' directory found in current working dir.\n"
     ]
    }
   ],
   "source": [
    "from vector_pipeline.settings import DATA_PATH\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"DATA_PATH as defined:\", DATA_PATH)\n",
    "print(\"Absolute path:\", DATA_PATH.resolve())\n",
    "print(\"Exists?\", DATA_PATH.exists())\n",
    "\n",
    "# Also list what's in ./data to see what you actually have\n",
    "data_dir = Path(\"data\")\n",
    "print(\"\\nContents of ./data directory:\")\n",
    "if data_dir.exists():\n",
    "    for p in data_dir.iterdir():\n",
    "        print(\" -\", p)\n",
    "else:\n",
    "    print(\"No 'data' directory found in current working dir.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9098657d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading vectorstore (or building if missing)...\n",
      "No existing DB found. Building Chroma vectorstore ...\n",
      "Loading data from data/processed_product.pkl ...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/processed_product.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mLoading vectorstore (or building if missing)...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m vs = \u001b[43mbuild_or_load_vectorstore\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mVectorstore is ready! üöÄ\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/ML/project-25-2-scrum-team-data/vector_pipeline/ingestion.py:179\u001b[39m, in \u001b[36mbuild_or_load_vectorstore\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    176\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m vectorstore\n\u001b[32m    178\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mNo existing DB found. Building Chroma vectorstore ...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbuild_chroma_vectorstore\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/ML/project-25-2-scrum-team-data/vector_pipeline/ingestion.py:125\u001b[39m, in \u001b[36mbuild_chroma_vectorstore\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    114\u001b[39m \u001b[33;03mEnd-to-end pipeline to build and persist the Chroma DB from scratch:\u001b[39;00m\n\u001b[32m    115\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    122\u001b[39m \u001b[33;03mThis will delete any existing Chroma directory first.\u001b[39;00m\n\u001b[32m    123\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    124\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLoading data from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDATA_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m ...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m df = \u001b[43mload_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    126\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLoaded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(df)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m rows.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    128\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mConverting rows to Documents ...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/ML/project-25-2-scrum-team-data/vector_pipeline/ingestion.py:43\u001b[39m, in \u001b[36mload_dataframe\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_dataframe\u001b[39m() -> pd.DataFrame:\n\u001b[32m     37\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     38\u001b[39m \u001b[33;03m    Load the preprocessed product dataframe from DATA_PATH.\u001b[39;00m\n\u001b[32m     39\u001b[39m \n\u001b[32m     40\u001b[39m \u001b[33;03m    This file should already contain a `combined_text` column\u001b[39;00m\n\u001b[32m     41\u001b[39m \u001b[33;03m    (or whatever COMBINED_TEXT_COLUMN is set to).\u001b[39;00m\n\u001b[32m     42\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m     df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_pickle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDATA_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     45\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m COMBINED_TEXT_COLUMN \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m df.columns:\n\u001b[32m     46\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m     47\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExpected column \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCOMBINED_TEXT_COLUMN\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDATA_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     48\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mbut found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(df.columns)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     49\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/jupyter-env/lib/python3.12/site-packages/pandas/io/pickle.py:185\u001b[39m, in \u001b[36mread_pickle\u001b[39m\u001b[34m(filepath_or_buffer, compression, storage_options)\u001b[39m\n\u001b[32m    123\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    124\u001b[39m \u001b[33;03mLoad pickled pandas object (or any object) from file.\u001b[39;00m\n\u001b[32m    125\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    182\u001b[39m \u001b[33;03m4    4    9\u001b[39;00m\n\u001b[32m    183\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    184\u001b[39m excs_to_catch = (\u001b[38;5;167;01mAttributeError\u001b[39;00m, \u001b[38;5;167;01mImportError\u001b[39;00m, \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m185\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    186\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    187\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    188\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    189\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    190\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    191\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[32m    192\u001b[39m     \u001b[38;5;66;03m# 1) try standard library Pickle\u001b[39;00m\n\u001b[32m    193\u001b[39m     \u001b[38;5;66;03m# 2) try pickle_compat (older pandas version) to handle subclass changes\u001b[39;00m\n\u001b[32m    194\u001b[39m     \u001b[38;5;66;03m# 3) try pickle_compat with latin-1 encoding upon a UnicodeDecodeError\u001b[39;00m\n\u001b[32m    196\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    197\u001b[39m         \u001b[38;5;66;03m# TypeError for Cython complaints about object.__new__ vs Tick.__new__\u001b[39;00m\n\u001b[32m    198\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/jupyter-env/lib/python3.12/site-packages/pandas/io/common.py:882\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    873\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(\n\u001b[32m    874\u001b[39m             handle,\n\u001b[32m    875\u001b[39m             ioargs.mode,\n\u001b[32m   (...)\u001b[39m\u001b[32m    878\u001b[39m             newline=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    879\u001b[39m         )\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m882\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    883\u001b[39m     handles.append(handle)\n\u001b[32m    885\u001b[39m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'data/processed_product.pkl'"
     ]
    }
   ],
   "source": [
    "from vector_pipeline.ingestion import build_or_load_vectorstore\n",
    "\n",
    "print(\"Loading / building vectorstore...\")\n",
    "vs = build_or_load_vectorstore()\n",
    "print(\"Vectorstore ready ‚úÖ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa006b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_json(data):\n",
    "    print(json.dumps(data, indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38407ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_request = {\n",
    "    \"session_id\": \"S12345\",\n",
    "    \"user_id\": \"U9001\",\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"Show me wireless headphones with noise cancelling\"}\n",
    "    ],\n",
    "    \"top_k\": 4,\n",
    "    \"use_reranker\": True\n",
    "}\n",
    "\n",
    "print(\"üì• Incoming API request:\")\n",
    "print_json(api_request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c46bdfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = run_chat(\n",
    "    messages=deepcopy(api_request[\"messages\"]),\n",
    "    k=api_request.get(\"top_k\", 4),\n",
    "    use_reranker=api_request.get(\"use_reranker\", True)\n",
    ")\n",
    "\n",
    "api_response = {\n",
    "    \"status\": \"success\",\n",
    "    \"session_id\": api_request[\"session_id\"],\n",
    "    \"user_id\": api_request[\"user_id\"],\n",
    "    \"answer\": result[\"answer\"],\n",
    "    \"messages\": result[\"messages\"],  # updated history\n",
    "    \"retrieved\": result[\"retrieved\"]\n",
    "}\n",
    "\n",
    "print(\"üì§ API response:\")\n",
    "print_json(api_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e714c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = [\n",
    "    {\"role\": \"user\", \"content\": \"Recommend a gaming laptop\"},\n",
    "]\n",
    "\n",
    "session_id = \"S777\"\n",
    "user_id = \"U007\"\n",
    "\n",
    "print(\"üß© First turn ‚Äî incoming:\")\n",
    "print_json({\"session_id\": session_id, \"user_id\": user_id, \"messages\": history})\n",
    "\n",
    "result1 = run_chat(history)\n",
    "\n",
    "print(\"\\nü§ñ First answer:\")\n",
    "print(result1[\"answer\"])\n",
    "\n",
    "print(\"\\nüì§ API response (turn 1):\")\n",
    "print_json({\n",
    "    \"status\": \"success\",\n",
    "    \"session_id\": session_id,\n",
    "    \"user_id\": user_id,\n",
    "    \"answer\": result1[\"answer\"],\n",
    "    \"messages\": result1[\"messages\"],\n",
    "    \"retrieved\": result1[\"retrieved\"]\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaabe59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "history2 = result1[\"messages\"] + [\n",
    "    {\"role\": \"user\", \"content\": \"What about something cheap but still good?\"}\n",
    "]\n",
    "\n",
    "print(\"üß© Second turn ‚Äî incoming:\")\n",
    "print_json({\"session_id\": session_id, \"user_id\": user_id, \"messages\": history2})\n",
    "\n",
    "result2 = run_chat(history2)\n",
    "\n",
    "print(\"\\nü§ñ Second answer:\")\n",
    "print(result2[\"answer\"])\n",
    "\n",
    "print(\"\\nüì§ API response (turn 2):\")\n",
    "print_json({\n",
    "    \"status\": \"success\",\n",
    "    \"session_id\": session_id,\n",
    "    \"user_id\": user_id,\n",
    "    \"answer\": result2[\"answer\"],\n",
    "    \"messages\": result2[\"messages\"],\n",
    "    \"retrieved\": result2[\"retrieved\"]\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c12ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"üìö Retrieved chunks: {len(result2['retrieved'])}\")\n",
    "\n",
    "for i, chunk in enumerate(result2[\"retrieved\"], start=1):\n",
    "    print(f\"\\n--- Chunk {i} ---\")\n",
    "    print(\"Metadata:\", chunk[\"metadata\"])\n",
    "    print(\"Snippet:\", chunk[\"snippet\"][:250], \"...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3ca236",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_once(session_id, user_id, user_input, history=None):\n",
    "    if history is None:\n",
    "        history = [{\"role\": \"user\", \"content\": user_input}]\n",
    "    else:\n",
    "        history = history + [{\"role\": \"user\", \"content\": user_input}]\n",
    "    \n",
    "    # simulate backend request\n",
    "    req = {\n",
    "        \"session_id\": session_id,\n",
    "        \"user_id\": user_id,\n",
    "        \"messages\": history,\n",
    "    }\n",
    "    \n",
    "    res = run_chat(req[\"messages\"])\n",
    "    \n",
    "    print(\"\\nüó£Ô∏è User:\", user_input)\n",
    "    print(\"ü§ñ Bot:\", res[\"answer\"])\n",
    "    return res[\"messages\"]\n",
    "\n",
    "# Example usage:\n",
    "history = chat_once(\"S999\", \"UABC\", \"What is a good smartphone for photography?\")\n",
    "history = chat_once(\"S999\", \"UABC\", \"And something cheaper?\", history)\n",
    "history = chat_once(\"S999\", \"UABC\", \"Which one has best battery?\", history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428781c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8cf35c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter-env (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
